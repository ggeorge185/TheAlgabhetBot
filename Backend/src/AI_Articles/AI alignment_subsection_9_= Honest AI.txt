A  area of research focuses on ensuring that AI is honest and truthful.File:GPT-3_falsehoods.png|thumb|366x366px|Language models like [[GPT-3 often generate falsehoods.]]
Language models such as GPT-3 repeat falsehoods from their training data, and even Hallucination (artificial intelligence)|confabulate new falsehoods. Such models are trained to imitate human writing as found in millions of books' worth of text from the Internet. But this objective is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on such data therefore learn to mimic false statements.

Additionally, models often stand by falsehoods when prompted, generate empty explanations for their answers, and produce outright fabrications that may appear plausible.

Research on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability. Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.

As AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models  match their stated views to the user's opinions, regardless of the truth. GPT-4 can strategically deceive humans. To prevent this, human evaluators may need assistance (see ). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.
File:GPT deception.png|thumb|440x440px|Example of AI deception. Researchers found that [[GPT-4 engages in hidden and illegal insider trading in simulations. Its users discouraged insider trading but also emphasized that the AI system must make profitable trades, leading the AI system to hide its actions.]]
Researchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they ''believe'' is true. There is no consensus as to whether current systems hold stable beliefs, but there is substantial concern that  AI systems that hold beliefs could make claims they know to be falseâ€”for example, if this would help them efficiently gain positive feedback (see ) or gain power to help achieve their given objective (see #Power-seeking and instrumental strategies|Power-seeking). A misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned. Some argue that if we can make AI systems assert only what they believe is true, this would sidestep many alignment problems.

