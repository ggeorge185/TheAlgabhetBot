In a July 2020 review in ''The New York Times'', Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just "amazing", "spooky", and "humbling", but also "more than a little terrifying".
* ''Daily Nous'' presented a series of articles by nine philosophers on GPT-3. Australian philosopher David Chalmers described GPT-3 as "one of the most interesting and important AI systems ever produced".
* A review in ''Wired (magazine)|Wired'' said that GPT-3 was "provoking chills across Silicon Valley".
* The ''National Law Review'' said that GPT-3 is an "impressive step in the larger process", with OpenAI and others finding "useful applications for all of this power" while continuing to "work toward a more Artificial general intelligence|general intelligence".
* An article in the ''MIT Technology Review,'' co-written by Deep Learning critic Gary Marcus, stated that GPT-3's "comprehension of the world is often seriously off, which means you can never really trust what it says." According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word.
* Jerome Pesenti, head of the Facebook AI lab, said GPT-3 is "unsafe," pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.
* Nabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use. As expected, GPT-3 showed several limitations. For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.
* Noam Chomsky expressed his skepticism about GPT-3's scientific value: "It's not a language model. It works just as well for impossible languages as for actual languages. It is therefore refuted, if intended as a language model, by normal scientific criteria. [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally."
* Luciano Floridi and Massimo Chiriatti highlighted the risk of "cheap production of good, semantic artefacts".
* OpenAI's Sam Altman himself criticized what he called "GPT-3 hype", acknowledging GPT-3 "has serious weakness and sometimes makes very silly mistakes... AI is going to change the world, but GPT-3 is just a very early glimpse."

