further|Algorithmic transparency}}

Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results. Such solutions include the consideration of the "right to understanding" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed. Toward this end, a movement for "Explainable artificial intelligence|Explainable AI" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias. PricewaterhouseCoopers|Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.

An initial approach towards transparency included the Open-source software|open-sourcing of algorithms. Software code can be looked into and improvements can be proposed through Comparison of source-code-hosting facilities|source-code-hosting facilities. However, this approach doesn't necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesn't understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience.

