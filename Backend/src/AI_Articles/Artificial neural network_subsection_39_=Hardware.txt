Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a Graph (discrete mathematics)|graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of Random-access memory|memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which require enormous Central processing unit|CPU power and time.

JÃ¼rgen Schmidhuber|Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by General-purpose computing on graphics processing units|GPGPUs (on Graphics processing unit|GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.

