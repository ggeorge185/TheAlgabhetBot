The simplest AI applications can be divided into two types: classifiers (e.g. "if shiny then diamond"), on one hand, and controllers (e.g. "if diamond then pick up"), on the other hand. Classifier (mathematics)|Classifiers
are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an "random variate|observation") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the "most widely used learner" at Google, due in part to its scalability.
Artificial neural network|Neural networks are also used as classifiers.

Learning algorithms for neural networks use local search (optimization)|local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.
Neural networks learn to model complex relationships between inputs and outputs and Pattern recognition|find patterns in data. In theory, a neural network can learn any function.

In feedforward neural networks the signal passes in only one direction.
Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.
Perceptrons
use only a single layer of neurons, deep learning



