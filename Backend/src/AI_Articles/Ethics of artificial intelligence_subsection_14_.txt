Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.

Bias can creep into algorithms in many ways. The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon's AI-powered recruitment tool was trained with its own recruitment data accumulated over the years, during which time the candidates that successfully got the job were mostly white males. Consequently, the algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turn out to be biased against female and minority candidates. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus â€” the source material the algorithm uses to learn about the relationships between different words.

Large companies such as IBM, Google, etc. that provide significant funding for research and development, have made efforts to research and address these biases. One solution for addressing bias is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.

The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some experts warn that algorithmic bias is already pervasive in many industries and that almost no one is making an effort to identify or correct it. There are some open-sourced tools by civil societies that are looking to bring more awareness to biased AI. There are however some limitations to the current landscape of fairness (machine learning)#Limitations|fairness in AI, due e.g. to the intrinsic ambiguities in the concept of discrimination, both at philosophical and legal level.

AI is also being incorporated into the hiring processes for almost every major company. There are many examples of certain characteristics that the AI is less likely to choose. Including the association between typically white names being more qualified, and the exclusion of anyone who went to a women's college. Facial recognition is also proven to be highly biased against those with darker skin tones. The word Muslims is shown to be more highly associated with violence than any other religions. Often times being able to easily detect the faces of white people while being unable to register the faces of people who are black. This is even more disconcerting considering the unproportionate use of security cameras and surveillance in communities that have high percentages of black or brown people. This fact has even been acknowledged in some states and led to the ban of police usage of AI materials or software. Even within the justice system AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. Often AI struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally. The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. A good example of this being if a Facial recognition system was only tested on people who were white then it would only have the data and face scans of white people making it much harder for it to interpret the facial structure and tones of other races and Ethnicity|ethnicities. To stop these biases there is not one single answer that can be used. The most useful approach has seemed to be the use of Data science|data scientists, Ethics|ethicists and other policymakers to improve AI's problems with biases. Often times the reasons for biases within AI is the data behind the program rather than the algorithm of the bot itself. AI's information is often times pulled from past human decisions or inequalities that can lead to biases in the decision-making processes for that bot.

Injustice in the use of AI will be much harder to eliminate within healthcare system as often times diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race. This can be perceived as a bias because each patient is a different case and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what is considered a biased decision on who receives what treatment. While it is known that there are differences in how diseases and injuries effect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there is already certain tests for diseases, such as Breast cancer, that is recommended to a certain group of people over others because they are more likely to contract it. If AI implements these statistics and applies them to each patient, it could be considered biased.

Examples of AI being proven to have bias include when the system used to predict which Defendents would be more likely to commit crimes in the future, COMPAS (software)|COMPAS, was found to predict higher risk values for black people than what their actual risk was. Another example being within Google's ads which targeted men with higher paying jobs and women with lower paying jobs. Often times it can be hard to detect AI biases within an Algorithm as often times it is not linked to the actual words associated with bias but rather words that biases can be affected by. An example of this being a person's residential area which can be used to link them to a certain group. This can lead to problems as often times businesses can avoid legal action through this loophole. This being because of the specific laws regarding the verbiage that is considered discriminatory by governments enforcing these policies.

