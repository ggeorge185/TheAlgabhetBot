A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that "perceptron may eventually be able to learn, make decisions, and translate languages." An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Marvin Minsky|Minsky and Seymour Papert|Papert's 1969 book ''Perceptrons (book)|Perceptrons''. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was funded in connectionism for 10 years.

Of the main efforts towards neural networks, Rosenblatt attempted to gather funds for building larger perceptron machines, but died in a boating accident in 1971. Minsky (of SNARC) turned to a staunch objector to pure connectionist AI. Widrow (of ADALINE) turned to adaptive signal processing, using techniques based on the Least mean squares|LMS algorithm. The SRI group (of MINOS) turned to symbolic AI and robotics. The main issues were lack of funding and the inability to train multilayered networks (backpropagation was unknown). The competition for government funding ended with the victory of symbolic AI approaches.
In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution (logic)|resolution and unification (computing)|unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and  who created the successful logic programming language Prolog.
Prolog uses a subset of logic (Horn clauses, closely related to "rules" and "Production system (computer science)|production rules") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar (cognitive architecture)|Soar and their unified theories of cognition.

Critics of the logical approach noted, as Hubert Dreyfus|Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Cathcart Wason|Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.
McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problemsâ€”not machines that think as people do.

