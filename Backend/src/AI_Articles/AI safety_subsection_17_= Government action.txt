Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to “rush to regulate in ignorance.” Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.

Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to “assure that systems are aligned with goals and values, including safety, robustness and trustworthiness." Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when "catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed."

In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, which states the British government "takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously." The strategy describes actions to assess long-term AI risks, including catastrophic risks.

Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan horse (computing)|Trojan attacks on AI systems. The DARPA engages in research on explainable artificial intelligence and improving robustness against Adversarial machine learning|adversarial attacks. And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.

