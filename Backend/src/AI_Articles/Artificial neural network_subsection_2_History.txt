main|History of artificial neural networks}}

The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Adrien-Marie Legendre|Legendre (1805) and Gauss (1795) for the prediction of planetary movement.

Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.

Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. In the late 1940s, Donald O. Hebb|D. O. Hebb created a learning hypothesis based on the mechanism of Neuroplasticity|neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called "calculators", to simulate a Hebbian network. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artificial neural network, funded by the United States Office of Naval Research.

The invention of the perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding into deep learning research. This led to "the golden age of AI" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example, in 1957 Herbert Simon famously said: who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This, along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not "produced the major impact that was then promised," shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI winter|AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned useful Knowledge representation|internal representations to classify non-linearily separable pattern classes. SOMs are neurophysiologically inspired neural networks that learn dimensionality reduction|low-dimensional representations of high-dimensional data while preserving the topology|topological structure of the data. They are trained using competitive learning. He called it the neocognitron. In 1969, he also introduced the rectifier (neural networks)|ReLU (rectified linear unit) activation function. CNNs have become an essential tool for computer vision.

The backpropagation algorithm is an efficient application of the Gottfried Wilhelm Leibniz|Leibniz chain rule (1673) to networks of differentiable nodes. and Arthur E. Bryson|Bryson had dynamic programming based continuous precursors of backpropagation already in 1960â€“61 in the context of control theory. 
In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.

The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988, Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989, Yann LeCun et al. trained a CNN to Handwriting recognition|recognize handwritten ZIP codes on mail. 
In 1992, Convolutional neural network#Pooling layer|max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. 
LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.

From 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on ''profiles'' (matrices) produced by multiple sequence alignments.

In the 1980s, backpropagation did not work well for deep FNNs and RNNs. To overcome this problem, Juergen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn Knowledge representation|internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be ''collapsed'' into a single RNN, by Knowledge distillation|distilling a higher level ''chunker'' network into a lower level ''automatizer'' network.

In 1992, Juergen Schmidhuber also published an ''alternative to RNNs'' which is now called a ''linear Transformer (machine learning model)|Transformer'' or a  Transformer with linearized Attention (machine learning)|self-attention a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns ''FROM'' and ''TO'' (which are now called ''key'' and ''value'' for Attention (machine learning)|self-attention). This fast weight ''attention mapping'' is applied to a query pattern.

The modern Transformer (machine learning model)|Transformer was introduced by Ashish Vaswani et al. in their 2017 paper "Attention Is All You Need." 
It combines this with a softmax operator and a projection matrix. Many modern large language models such as ChatGPT, GPT-4, and BERT (language model)|BERT use it. Transformers are also increasingly being used in computer vision.

In 1991, Juergen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called "artificial curiosity."

In 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.
Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras, Timo Aila, Samuli  Laine, and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion.

Sepp Hochreiter's diploma thesis (1991) was called "one of the most important documents in the history of machine learning" by his supervisor Juergen Schmidhuber. and proposed recurrent Residual neural network|residual connections to solve it. This led to the deep learning method called long short-term memory (LSTM), published in Neural Computation (1997). LSTM recurrent neural networks can learn "very deep learning" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The "vanilla LSTM" with forget gate was introduced in 1999 by Felix Gers, Juergen Schmidhuber|Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet Competition|ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.

Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.

Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Andrew Ng|Ng and Jeff Dean (computer scientist)|Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as "deep learning".

Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. For example, the bi-directional and multi-dimensional long short-term memory (LSTM)

Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012).

