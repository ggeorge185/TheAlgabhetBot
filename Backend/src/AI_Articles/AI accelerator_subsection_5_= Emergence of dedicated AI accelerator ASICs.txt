While GPUs and FPGAs perform far better than CPUs for AI-related tasks, a factor of up to 10 in efficiency may be gained with a more specific design, via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized cache-aware model|memory use and the use of minifloat|lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision floating-point format|half-precision and the bfloat16 floating-point format. Companies such as Google, Qualcomm, Amazon, Apple, Facebook, AMD and Samsung are all designing their own AI ASICs. Cerebras|Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2), to support deep learning workloads.

