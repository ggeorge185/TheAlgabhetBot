Artificial intelligence and computational cognitive science researchers have long attempted to computationally model human's ability to reason about the (false) beliefs of others in tasks like the Sally-Anne test. Many approaches have been taken to replicate this ability in computers, including neural network approaches, epistemic plan recognition, and Bayesian theory-of-mind. These approaches typically model agents as rationally selecting actions based on their beliefs and desires, which can be used to either predict their future actions (as in the Sally-Anne test), or to infer their current beliefs and desires. In constrained settings, these models are able to reproduce human-like behavior on tasks similar to the Sally-Anne test, provided that the tasks are represented in a machine-readable format.

On March 22, 2023, a research team from Microsoft released a paper showing that the Large language model|LLM-based AI system GPT-4 could pass an instance of the Sallyâ€“Anne test, which the authors interpret as "suggest[ing] that GPT-4 has a very advanced level of theory of mind." However, the generality of this finding has been disputed by several other papers, which indicate that GPT-4's ability to reason about the beliefs of other agents remains limited (59% accuracy on the [https://aclanthology.org/D19-1598/ ToMi benchmark]), and is not robust to "adversarial" changes to the Sally-Anne test that humans flexibly handle. While some authors argue that the performance of GPT-4 on Sally-Anne-like tasks can be increased to 100% via improved prompting strategies, this approach appears to improve accuracy to only 73% on the larger ToMi dataset. and that they do not reliably produce graded inferences about the goals of other agents from observed actions. The degree to which LLMs such as GPT-4 can perform social reasoning thus remains an active area of research.

