Some scientists, such as Stephen Hawking and Stuart J. Russell|Stuart Russell, have articulated concerns that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable "intelligence explosion" could lead to human extinction. Co-founder Musk characterizes AI as humanity's "biggest existential threat".

Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence. OpenAI states that AI "should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible."

Vishal Sikka, the former CEO of Infosys, stated that an "openness" where the endeavor would "produce results generally in the greater interest of humanity" was a fundamental requirement for his support, and that OpenAI "aligns very nicely with our long-held values" and their "endeavor to do purposeful work". Cade Metz of ''Wired'' suggests that corporations such as Amazon.com|Amazon may be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook which own enormous supplies of proprietary data. Altman states that Y Combinator companies will share their data with OpenAI.

Musk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with existential risk from artificial intelligence. Philosopher Nick Bostrom is skeptical of Musk's approach: "If you have a button that could do bad things to the world, you don't want to give it to everyone." During a 2016 conversation about technological singularity, Altman said "we don't plan to release all of our source code" and mentioned a plan to "allow wide swaths of the world to elect representatives to a new governance board". Greg Brockman stated "Our goal right now... is to do the best thing there is to do. It's a little vague."

Conversely, OpenAI's initial decision to withhold GPT-2 around 2019, due to a wish to "err on the side of caution" in the presence of potential misuse was criticized by advocates of openness. Delip Rao, an expert in text generation, stated "I don't think  spent enough time proving  was actually dangerous." Other critics argued that open publication is necessary to replicate the research and to be able to come up with countermeasures.

More recently, in 2022, OpenAI published its approach to the AI alignment|alignment problem. They expect that aligning Artificial general intelligence|AGI to human values is likely harder than aligning current AI systems: "Unaligned AGI could pose substantial risks to humanity and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together". They explore how to better use human feedback to train AI systems. They also consider using AI to incrementally automate alignment research. After the reorganization of November 2023, the return of Altman as CEO and the composition of the new Board of Directors without Sutskever indicate a probable shift in strategy towards a stronger business focus and reduced influence of cautious people at OpenAI.

OpenAI claims that it's developed a way to use GPT-4, its flagship generative AI model, for content moderation â€” lightening the burden on human teams.

