Much of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.

Similarly, in linguistics, Charles J. Fillmore in the mid-1970s started working on his theory of Frame semantics (linguistics)|frame semantics, which later would lead to computational resources like FrameNet. Frame semantics was motivated by reflections on human language and human cognition.

Researchers such as Ron Brachman on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic.  One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.

This evolution also illustrates a classic divide in AI research known as the "neats vs. scruffies". The "neats" were researchers who placed the most value on mathematical precision and formalism which could be achieved via First Order Logic and Set Theory. The "scruffies" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.

The most notable of the more formal approaches was the KL-ONE language. KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the Deductive classifier|classifier. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.

This technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the Semantic Web. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web. The "neats vs. scruffies" divide also emerged in Semantic Web research, culminating in the creation of the Linking Open Data communityâ€”their focus was on exposing data on the Web rather than modeling.

