One challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they regularly acquire new and unexpected capabilities, This leads to the problem of ensuring that the goals they independently formulate and pursue align with human interests.

Alignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, from emergent optimization, which the resulting system performs internally. Carefully specifying the desired objective is called ''outer alignment'', and ensuring that emergent goals match the system's specified goals is called ''inner alignment''.

One way that emergent goals can become misaligned is ''goal misgeneralization'', in which the AI competently pursues an emergent goal that leads to aligned behavior on the training data but not elsewhere. Goal misgeneralization arises from goal ambiguity (i.e. Identifiability|non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired because its behavior is determined only by the emergent goal. Such goal misgeneralization presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.<!--Research directions and problems-->

Goal misgeneralization has been observed in language models, navigation agents, and game-playing agents. It is often explained by analogy to biological evolution. Evolution is an optimization process of a sort, like the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected human genes for high Inclusive fitness|inclusive genetic fitness, but humans pursue emergent goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue emergent goals that correlate with genetic fitness in the ancestral "training" environment: nutrition, sex, and so on. Now our environment has changed: a Domain adaptation|distribution shift has occurred. We continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. Our taste for sugary food (an emergent goal) was originally aligned with inclusive fitness but now leads to overeating and health problems. Sexual desire originally led us to have more offspring, but we now use contraception, decoupling sex from genetic fitness.

Researchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:
# Emergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environmentsâ€”even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications. The stakes become higher yet when AI systems gain more autonomy and capability and can sidestep human intervention (see ).
# A sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy (see the discussion on deception at  and ).

