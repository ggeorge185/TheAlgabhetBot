see also|AI alignment|Machine ethics|Friendly artificial intelligence|Regulation of artificial intelligence}}

Many scholars concerned about the AGI existential risk believe that the best approach is to conduct substantial research into solving the difficult "control problem": what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly manner after it reaches superintelligence? Social measures may mitigate the AGI existential risk; for instance, one recommendation is for a UN-sponsored "Benevolent AGI Treaty" that would ensure only altruistic AGIs be created. Similarly, an arms control approach has been suggested, as has a global peace treaty grounded in the international relations theory of conforming instrumentalism, with an artificial superintelligence potentially being a signatory.<!-- in physica_scripta, see sections 3.3.2. Encourage Research into Safe AGI, and 3.3.3. Differential Technological Progress -->

Researchers at Google have proposed research into general "AI safety" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI. A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion.<!-- Precipice chapter=Chapter 2: Existential Risk|at=Footnote 55 --> Bostrom suggests that funding of protective technologies should be prioritized over potentially dangerous ones.<!-- Precipice chapter=Chapter 7: Safeguarding Humanity|at=Kindle loc 3327 --> Some funders, such as Musk, propose that radical human enhancement|human cognitive enhancement could be such a technology, for example direct neural linking between human and machine; others argue that enhancement technologies may themselves pose an existential risk. Researchers could closely monitor or attempt to "box in" an initial AI at a risk of becoming too powerful. A dominant superintelligent AI, if aligned with human interests, might itself take action to mitigate the risk of takeover by rival AI, although the creation of the dominant AI could itself pose an existential risk.

Institutions such as the Alignment Research Center, the Machine Intelligence Research Institute, the Future of Humanity Institute, the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI are involved in research into AI risk and safety.

