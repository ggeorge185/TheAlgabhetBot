Since ''Q''-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as "optimistic initial conditions", can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward <math>r</math> can be used to reset the initial conditions. According to this idea, the first time an action is taken the reward is used to set the value of <math>Q</math>. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates ''reset of initial conditions'' (RIC) is expected to predict participants' behavior better than a model that assumes any ''arbitrary initial condition'' (AIC). This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.

One solution is to use an (adapted) artificial neural network as a function approximator. Another possibility is to integrate Fuzzy Rule Interpolation (FRI) and use sparse Fuzzy rule|fuzzy rule-bases instead of discrete Q-tables or ANNs, which has the advantage of being a human-readable knowledge representation form. Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.

