Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or "formal"—reasoning has a long history. Chinese Philosophy|Chinese, Indian philosophy|Indian, and Greek Philosophy|Greek philosophers all developed structured methods of formal deduction in the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose ''Euclid's Elements|Elements'' was a model of formal reasoning), Muhammad ibn Musa al-Khwarizmi|al-Khwārizmī (who developed algebra and gave his name to "algorithm") and European Scholasticism|scholastic philosophers such as William of Ockham and Duns Scotus.

Spanish philosopher Ramon Llull (1232–1315) developed several ''logical machines'' devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.

File:Gottfried Wilhelm Leibniz, Bernhard Christoph Francke.jpg|left|thumb|upright|[[Gottfried Leibniz, who speculated that human reason could be reduced to mechanical calculation]]
In the 17th century, Gottfried Leibniz|Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan (Hobbes book)|''Leviathan'': "reason is nothing but reckoning". Gottfried Leibniz|Leibniz envisioned a universal language of reasoning, the ''characteristica universalis'', which would reduce argumentation to calculation so that "there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): ''Let us calculate''." These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.

In the 21th century, the study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as George Boole|Boole's ''The Laws of Thought'' and Frege's ''Begriffsschrift''. Building on Frege's system, Bertrand Russell|Russell and Alfred North Whitehead|Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the ''Principia Mathematica'' in 1913. Inspired by Bertrand Russell|Russell's success, Hilbert's program|David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: "can all of mathematical reasoning be formalized?" His question was answered by Kurt Gödel|Gödel's Gödel's incompleteness theorems|incompleteness proof, Alan Turing|Turing's Turing machine|machine and Alonzo Church|Church's Lambda calculus.

File:Classic shot of the ENIAC.jpg|right|thumbnail|250px|US Army photo of the ENIAC at the Moore School of Electrical Engineering
Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, ''any'' form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.

