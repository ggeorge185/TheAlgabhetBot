Predictive policing faces issues that affect its effectiveness. Obioha mentions several concerns raised about predictive policing. High costs and limited use prevent more widespread use, especially among poorer countries. Another issue that affects predictive policing is that it relies on human input to determine patterns. Flawed data can lead to biased and possibly racist results. Technology cannot predict crime, it can only weaponize proximity to policing. Though it is claimed to be unbiased data, communities of color and low income are the most targeted.  It should also be noted that not all crime is reported, making the data faulty and inaccurate.

In 2020, following George Floyd protests|protests against police brutality, a group of mathematicians published a letter in ''Notices of the American Mathematical Society'' urging colleagues to stop work on predictive policing. Over 1,500 other mathematicians joined the proposed boycott.

Some applications of predictive policing have targeted minority neighborhoods and lack feedback loops.

Cities throughout the United States are enacting legislation to restrict the use of predictive policing technologies and other “invasive” intelligence-gathering techniques within their jurisdictions.

Following the introduction of predictive policing as a crime reduction strategy, via the results of an algorithm created through the use of the software PredPol, the city of Santa Cruz, California experienced a decline in the number of burglaries reaching almost 20% in the first six months the program was in place. Despite this, in late June 2020 in the aftermath of the murder of George Floyd in Minneapolis|Minneapolis, Minnesota along with a growing call for increased accountability amongst police departments, the Santa Cruz City Council voted in favor of a complete ban on the use of predictive policing technology.

Accompanying the ban on predictive policing, was a similar prohibition of Facial-recognition technology|facial recognition technology. Facial recognition technology has been criticized for its reduced accuracy on darker skin tones - which can contribute to cases of mistaken identity and potentially, wrongful convictions.

In 2019, Michael Oliver, of Detroit|Detroit, Michigan, was wrongfully accused of larceny when his face registered as a “match” in the DataWorks Plus software to the suspect identified in a video taken by the victim of the alleged crime. Oliver spent months going to court arguing for his innocence - and once the judge supervising the case viewed the video footage of the crime, it was clear that Oliver was not the perpetrator. In fact, the perpetrator and Oliver did not resemble each other at all  - except for the fact that they are both African-American which makes it more likely that the facial recognition technology will make an identification error.

For example, as Dorothy Roberts explains in her academic journal article, Digitizing the Carceral State, the data entered into predictive policing algorithms to predict where crimes will occur or who is likely to commit criminal activity, tends to contain information that has been impacted by racism. For example, the inclusion of arrest or incarceration history, neighborhood of residence, level of education, membership in gangs or organized crime groups, 9-1-1|911 call records, among other features, can produce algorithms that suggest the over-policing of Minority group|minority or Low income|low-income communities.<ref name="Wikipedia"/>

