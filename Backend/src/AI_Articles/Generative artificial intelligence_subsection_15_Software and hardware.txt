Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, Text-to-image model|text-to-image products such as Midjourney, and text-to-video products such as Runway (company)|Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office, Google Photos, and Adobe Photoshop. Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model.

Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11.

Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require AI accelerator|accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.

The advantages of running generative AI locally include protection of Information privacy|privacy and intellectual property, and avoidance of rate limiting and censorship. The Reddit|subreddit r/LocalLLaMA in particular focuses on using Consumer electronics|consumer-grade gaming graphics cards through such techniques as Large language model#Compression|compression. That forum is one of only two sources Andrej Karpathy trusts for Language model#Evaluation and benchmarks|language model benchmarks. Yann LeCun has advocated open-source models for their value to Vertical market software|vertical applications and for improving AI safety.

Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's Hopper (microarchitecture)|H100) or AI accelerator chips (such as Google's Tensor Processing Unit|TPU). These very large models are typically accessed as Cloud computing|cloud services over the Internet.

In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the NVIDIA A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions.

There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it.

