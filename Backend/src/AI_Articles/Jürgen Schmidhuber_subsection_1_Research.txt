In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths in artificial neural networks. To overcome this problem, Schmidhuber (1991) proposed a hierarchy of recurrent neural networks (RNNs) pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn Knowledge representation|internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be ''collapsed'' into a single RNN, by Knowledge distillation|distilling a higher level ''chunker'' network into a lower level ''automatizer'' network.

In 1991, Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. and called it "one of the most important documents in the history of machine learning". 
The standard LSTM architecture which is used in almost all current applications 
was introduced in 2000 by Felix Gers, Schmidhuber, and Fred Cummins. Today's "vanilla LSTM" using backpropagation through time was published with his student Alex Graves (computer scientist)|Alex Graves in 2005, and its connectionist temporal classification (CTC) training algorithm in 2006. CTC enabled end-to-end speech recognition with LSTM. By the 2010s, the LSTM became the dominant technique for a variety of natural language processing tasks including speech recognition and machine translation, and was widely implemented in commercial technologies such as Google Translate and Siri. LSTM has become the  most cited neural network of the 20th century. 7 months later, the ImageNet 2015 competition was won with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century. through large language models such as ChatGPT. As early as 1992, Schmidhuber published an alternative to recurrent neural networks which is now called a Transformer with linearized Attention_(machine_learning)|self-attention (save for a normalization operator). It learns ''internal spotlights of attention'': a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns ''FROM'' and ''TO'' (which are now called ''key'' and ''value'' for Attention_(machine_learning)|self-attention). This fast weight ''attention mapping'' is applied to a query pattern.  

In 2011, Schmidhuber's team at IDSIA with his postdoc Dan Ciresan also achieved dramatic speedups of convolutional neural networks (CNNs) on fast parallel computers called GPUs. An earlier CNN on GPU by Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU. The deep CNN of Dan Ciresan et al. (2011) at IDSIA was already 60 times faster and achieved the first superhuman performance in a computer vision contest in August 2011. Between 15 May 2011 and 10 September 2012, their fast and deep CNNs won no fewer than four image competitions. They also significantly improved on the best performance in the literature for multiple image databases. The approach has become central to the field of computer vision. who applied the backpropagation algorithm to a variant of Kunihiko Fukushima's original CNN architecture called neocognitron, later modified by J. Weng's method called Convolutional neural network#Pooling layer|max-pooling. and published  details of numerous priority disputes with Hinton, Bengio and LeCun.

The term "schmidhubered" has been jokingly used in the AI community to describe Schmidhuber's habit of publicly challenging the originality of other researchers' work, a practice seen by some in the AI community as a "rite of passage" for young researchers. Some suggest that Schmidhuber's significant accomplishments have been underappreciated due to his confrontational personality.

