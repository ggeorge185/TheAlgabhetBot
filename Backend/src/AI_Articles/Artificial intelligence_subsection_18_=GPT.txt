Generative pre-trained transformer|Generative pre-trained transformers (GPT) are Large language model|large language models that are based on the semantic relationships between words in sentences (natural language processing). Text-based GPT models are pre-trained on a large corpus of text which can be from the internet. The pre-training consists in predicting the next Lexical analysis|token (a token being usually a word, subword, or punctuation). Throughout this pre-training, GPT models accumulate knowledge about the world, and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are still prone to generating falsehoods called "Hallucination (artificial intelligence)|hallucinations", although this can be reduced with RLHF and quality data. They are used in chatbot|chatbots which allow you to ask a question or request a task in simple text.

Current models and services include: Bard (chatbot)|Bard, ChatGPT, Anthropic#Claude|Claude, Microsoft Copilot|Copilot and LLaMA. Multimodal learning|Multimodal GPT models can process different types of data (Modality (humanâ€“computer interaction)|modalities) such as images, videos, sound and text.

