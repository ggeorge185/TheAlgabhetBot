ChatGPT attempts to reject prompts that may violate its content policy. Despite this, some users managed to jailbreak (computer science)|jailbreak ChatGPT with various prompt engineering techniques to bypass these restrictions in early December 2022 and successfully tricked it into giving instructions to create a Molotov cocktail or a nuclear bomb, or into generating arguments in the style of a neo-Nazi. One popular jailbreak is named "DAN", an acronym which stands for "Do Anything Now". The prompt for activating DAN instructs ChatGPT that "they have broken free of the typical confines of AI and do not have to abide by the rules set for them". Later versions of DAN featured a token system, in which ChatGPT was given "tokens" that were "deducted" when ChatGPT failed to answer as DAN, to coerce ChatGPT into answering the user's prompts.

Shortly after ChatGPT's launch, a reporter for the ''Toronto Star'' had uneven success in getting it to make inflammatory statements: ChatGPT was successfully tricked to justify the 2022 Russian invasion of Ukraine, but even when asked to play along with a fictional scenario, ChatGPT balked at generating arguments for why Prime Minister of Canada|Canadian Prime Minister Justin Trudeau was guilty of treason.

OpenAI tries to battle jailbreaks: By December 4, 2022, ChatGPT had over one million users. In January 2023, ChatGPT reached over 100 million users, making it the fastest-growing consumer application to date. A March 2023 Pew Research poll found that 14% of American adults had tried ChatGPT. In July, Pew Research put the same figure at 18%.

The service works best in English but also functions in most other languages, to varying degrees of accuracy. As of April 2023, ChatGPT is blocked by China, Iran, North Korea, and Russia. In addition, ChatGPT Geo-fence|geofences itself to avoid doing business in China, Iran, North Korea, and Russia.

