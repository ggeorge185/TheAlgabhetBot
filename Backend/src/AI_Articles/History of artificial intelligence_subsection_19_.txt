The McCulloch and Pitts paper (1944) inspired approaches to creating computing hardware that realizes the neural network approach to AI in hardware. The most influential was the effort led by Frank Rosenblatt on building Perceptron|Perceptron machines (1957-1962) of up to four layers. He was primarily funded by Office of Naval Research. Bernard Widrow and his student Marcian Hoff|Ted Hoff built ADALINE (1960) and ADALINE|MADALINE (1962), which had up to 1000 adjustable weights. A group at SRI International|Stanford Research Institute led by Charles Rosen (scientist)|Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by United States Army Signal Corps|U.S. Army Signal Corps. MINOS II had 6600 adjustable weights, and was controlled with an SDS 9 Series|SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran Computer programming in the punched card era|coding sheets.

Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers. The hardware diversity was particularly clear in the different technologies used in implementing the adjustable weights. The perceptron machines and the Stochastic Neural Analog Reinforcement Calculator|SNARC used potentiometers moved by electric motors. ADALINE used memistors adjusted by electroplating, though they also used simulations on an IBM 1620. The MINOS machines used Magnetic-core memory|ferrite cores with multiple holes in them that could be individually blocked, with the degree of blockage representing the weights.

Though there were multi-layered neural networks, most neural networks during this period had only one layer of adjustable weights. There were empirical attempts at training more than a single layer, but they were unsuccessful. Backpropagation did not become prevalent for neural network training until the 1980s.

A semantic net represents concepts (e.g. "house", "door") as nodes and relations among concepts (e.g. "has-a") as links between the nodes. The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.

Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a program (See ELIZA effect). But in fact, ELIZA had no idea what she was talking about. She simply gave a canned response or repeated back what was said to her, rephrasing her response with a few grammar rules. ELIZA was the first chatterbot.

