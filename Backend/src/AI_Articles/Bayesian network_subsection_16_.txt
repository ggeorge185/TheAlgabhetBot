This definition can be made more general by defining the "d"-separation of two nodes, where d stands for directional. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks. First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ''ɛ''&nbsp;<&nbsp;1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ''ɛ''&nbsp;<&nbsp;1/2 with confidence probability greater than&nbsp;1/2.

At about the same time, Dan Roth|Roth proved that exact inference in Bayesian networks is in fact Sharp-P-complete|#P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2<sup>''n''<sup>1−''ɛ''</sup></sup> for every ''ɛ'' > 0, even for Bayesian networks with restricted architecture, is NP-hard.

In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by <math>1/p(n)</math> where <math>p(n)</math> was any polynomial of the number of nodes in the network, <math>n</math>.

