The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neuron model|biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.

A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. He wanted to distinguish between two different hypotheses about artificial intelligence: }}
* '''Strong AI hypothesis''': An artificial intelligence system can have "a mind" and "consciousness".
* '''Weak AI hypothesis''': An artificial intelligence system can (only) ''act like'' it thinks and has a mind and consciousness.
The first one he called "strong" because it makes a ''stronger'' statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a "weak AI" machine would be precisely identical to a "strong AI" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.

In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term "strong AI" to mean "human level artificial general intelligence". According to Stuart J. Russell|Russell and Peter Norvig|Norvig, "as long as the program works, they don't care if you call it real or a simulation." If the program can behave ''as if'' it has a mind, then there is no need to know if it ''actually'' has mind â€“ indeed, there would be no way to tell. For AI research, Searle's "weak AI hypothesis" is equivalent to the statement "artificial general intelligence is possible". Thus, according to Russell and Norvig, "most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis." Thus, for academic AI research, "Strong AI" and "AGI" are two different things.

