main|Bootstrap aggregating}}File:Bootstrap set generation.png|thumb|upright=0.75|Three datasets bootstrapped from an original set. Example A occurs twice in set 1 because these are chosen with replacement.Bootstrap aggregation (''bagging'') involves training an ensemble on ''bootstrapped'' data sets. A bootstrapped set is created by selecting from original training data set with replacement. Thus, a bootstrap set may contain a given example zero, one, or multiple times. Ensemble members can also have limits on the features (e.g., nodes of a decision tree), to encourage exploring of diverse features. The variance of local information in the bootstrap sets and feature considerations promote diversity in the ensemble, and can strengthen the ensemble. To reduce overfitting, a member can be validated using the out-of-bag set (the examples that are not in its bootstrap set). 

Inference is done by '''voting''' of predictions of ensemble members, called '''aggregation'''. It is illustrated below with an ensemble of four decision trees. The query example is classified by each tree. Because three of the four predict the ''positive'' class, the ensemble's overall classification is ''positive''. random forest|Random forests like the one shown are a common application of bagging.

File:Ensemble Aggregation.png|frameless|center|An example of the aggregation process for an ensemble of decision trees. Individual classifications are aggregated, and an overall classification is derived.|615x615px

