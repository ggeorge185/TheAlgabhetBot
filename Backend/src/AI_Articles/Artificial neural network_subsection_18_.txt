In unsupervised learning, input data is given along with the cost function, some function of the data <math>\textstyle x</math> and the network's output. The cost function is dependent on the task (the model domain) and any ''A priori and a posteriori|a priori'' assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model <math>\textstyle f(x) = a</math> where <math>\textstyle a</math> is a constant and the cost <math>\textstyle C=E[(x - f(x))^2]</math>. Minimizing this cost produces a value of <math>\textstyle a</math> that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in Data compression|compression it could be related to the mutual information between <math>\textstyle x</math> and <math>\textstyle f(x)</math>, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general Approximation|estimation problems; the applications include Data clustering|clustering, the estimation of statistical distributions, Data compression|compression and Bayesian spam filtering|filtering.

