Main|Algorithmic bias}}Machine learning approaches in particular can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.

Language models learned from data have been shown to contain human-like biases. An experiment carried out by ProPublica, a predictive policing company, regarding machine learning algorithm’s insight towards the recidivism rates among prisoners falsely flagged “black defendants high risk twice as often as white defendants.” In 2015, Google photos would often tag black people as gorillas, Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.

Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for Fairness (machine learning)|fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that "There's nothing artificial about AI...It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility."

