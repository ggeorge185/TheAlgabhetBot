The General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses "Automated individual decision-making, including Profiling (information science)|profiling" in Article 22. These rules prohibit "solely" automated decisions which have a "significant" or "legal" effect on an individual, unless they are explicitly authorised by consent, contract, or Member state of the European Union|member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.

The GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in Recital (law)|recital 71, noting that<blockquote>the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate&nbsp;... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.</blockquote>Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of Recital (law)|recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law,

