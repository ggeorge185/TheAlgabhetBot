Regardless of the approach used, most natural-language-understanding systems share some common components. The system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable Ontology (information science)|ontology requires significant effort, ''e.g.'', the Wordnet lexicon required many person-years of effort.

The system also needs theory from ''semantics'' to guide the comprehension. The interpretation capabilities of a language-understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade-offs in their suitability as the basis of computer-automated semantic interpretation. These range from ''naive semantics'' or ''stochastic semantic analysis'' to the use of ''pragmatics'' to derive meaning from context. Semantic parsers convert natural-language texts into formal meaning representations.

Advanced applications of natural-language understanding also attempt to incorporate logical inference within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in predicate logic, then using logical deduction to arrive at conclusions. Therefore, systems based on functional languages such as Lisp (programming language)|Lisp need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language Prolog generally rely on an extension of the built-in logical representation framework.

The management of context (language use)|context in natural-language understanding can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the Formal semantics (natural language)|formal modeling of context, each with specific strengths and weaknesses.

