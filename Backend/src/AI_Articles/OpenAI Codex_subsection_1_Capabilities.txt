Based on GPT-3, a neural network trained on text, Codex was additionally trained on 159 gigabytes of Python (programming language)|Python code from 54 million GitHub repositories. A typical use case of Codex is for a user to type a comment, such as "<code>//compute the moving average of an array for a given window size</code>", then use the AI to suggest a block of code that satisfies that comment prompt. OpenAI stated that Codex can complete approximately 37% of requests and is meant to make human programming faster rather than to replace it. According to OpenAI's blog, Codex excels most at "mapping... simple problems to existing code", which they describe as "probably the least fun part of programming". Jeremy Howard (entrepreneur)|Jeremy Howard, co-founder of Fast.ai, stated that "Codex is a way of getting code written without having to write as much code", and that "it is not always correct, but it is just close enough". According to a paper written by OpenAI researchers, when Codex attempted each test case 100 times, it generated working solutions for 70.2% of prompts.

OpenAI claims that Codex can create code in over a dozen programming languages, including Go (programming language)|Go, JavaScript, Perl, PHP, Ruby (programming language)|Ruby, Shell (programming language)|Shell, Swift (programming language)|Swift, and TypeScript, though it is most effective in Python. Microsoft is  Codex's capabilities.

