File:HONDA ASIMO.jpg|thumb|[[ASIMO was an advanced humanoid robot developed by Honda. Shown here at Expo 2005.]]
Robots and artificial intelligences do not inherently contain or obey the Three Laws; their human creators must choose to program them in, and devise a means to do so. Robots already exist (for example, a Roomba) that are too simple to understand when they are causing pain or injury and know to stop. Many are constructed with physical safeguards such as bumpers, warning beepers, safety cages, or restricted-access zones to prevent accidents. Even the most complex robots currently produced are incapable of understanding and applying the Three Laws; significant advances in artificial intelligence would be needed to do so, and even if AI could reach human-level intelligence, the inherent ethical complexity as well as cultural/contextual dependency of the laws prevent them from being a good candidate to formulate robotics design constraints. However, as the complexity of robots has increased, so has interest in developing guidelines and safeguards for their operation.

In a 2007 guest editorial in the journal ''Science (journal)|Science'' on the topic of "Robot Ethics", SF author Robert J. Sawyer argues that since the United States Armed Forces|U.S. military is a major source of funding for robotic research (and already uses armed unmanned aerial vehicles to kill enemies) it is unlikely such laws would be built into their designs. In a separate essay, Sawyer generalizes this argument to cover other industries stating:

<blockquote>The development of AI is a business, and businesses are notoriously uninterested in fundamental safeguards — especially philosophic ones. (A few quick examples: the tobacco industry, the automotive industry, the nuclear industry. Not one of these has said from the outset that fundamental safeguards are necessary, every one of them has resisted externally imposed safeguards, and none has accepted an absolute edict against ever causing harm to humans.)</blockquote>

David Langford has suggested a tongue-in-cheek set of laws:

# A robot will not harm authorized Government personnel but will Terminate with extreme prejudice|terminate intruders with extreme prejudice.
# A robot will obey the orders of authorized personnel except where such orders conflict with the Third Law.
# A robot will guard its own existence with lethal antipersonnel weaponry, because a robot is bloody expensive.

Roger Clarke (aka Rodger Clarke) wrote a pair of papers analyzing the complications in implementing these laws in the event that systems were someday capable of employing them. He argued "Asimov's Laws of Robotics have been a very successful literary device. Perhaps ironically, or perhaps because it was artistically appropriate, the sum of Asimov's stories disprove the contention that he began with: It is not possible to reliably constrain the behaviour of robots by devising and applying a set of rules." On the other hand, Asimov's later novels ''The Robots of Dawn'', ''Robots and Empire'' and ''Foundation and Earth'' imply that the robots inflicted their worst long-term harm by obeying the Three Laws perfectly well, thereby depriving humanity of inventive or risk-taking behaviour.

In March 2007 the South Korean government announced that later in the year it would issue a "Robot Ethics Charter" setting standards for both users and manufacturers. According to Park Hye-Young of the Ministry of Information and Communication the Charter may reflect Asimov's Three Laws, attempting to set ground rules for the future development of robotics.

The futurist Hans Moravec (a prominent figure in the transhumanism|transhumanist movement) proposed that the Laws of Robotics should be adapted to "corporate intelligences" — the corporations driven by AI and robotic manufacturing power which Moravec believes will arise in the near future. design flaws or construction errors could functionally take the place of biological mutation.

In the July/August 2009 issue of ''IEEE Intelligent Systems'', Robin Murphy (Raytheon Professor of Computer Science and Engineering at Texas A&M) and David D. Woods (director of the Cognitive Systems Engineering Laboratory at Ohio State) proposed "The Three Laws of Responsible Robotics" as a way to stimulate discussion about the role of responsibility and authority when designing not only a single robotic platform but the larger system in which the platform operates. The laws are as follows:
# A human may not deploy a robot without the human-robot work system meeting the highest legal and professional standards of safety and ethics.
# A robot must respond to humans as appropriate for their roles.
# A robot must be endowed with sufficient situated autonomy to protect its own existence as long as such protection provides smooth transfer of control which does not conflict with the First and Second Laws.

Woods said, "Our laws are a little more realistic, and therefore a little more boring” and that "The philosophy has been, ‘sure, people make mistakes, but robots will be better – a perfect version of ourselves’. We wanted to write three new laws to get people thinking about the human-robot relationship in more realistic, grounded ways." 
# Robots are multi-use tools. Robots should not be designed solely or primarily to kill or harm humans, except in the interests of national security.
# Humans, not Robots, are responsible agents. Robots should be designed and operated as far as practicable to comply with existing laws, fundamental rights and freedoms, including privacy.
# Robots are products. They should be designed using processes which assure their safety and security.
# Robots are manufactured artefacts. They should not be designed in a deceptive way to exploit vulnerable users; instead their machine nature should be transparent.
# The person with legal responsibility for a robot should be attributed.

