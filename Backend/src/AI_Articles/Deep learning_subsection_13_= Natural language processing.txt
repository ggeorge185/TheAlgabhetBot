Main|Natural language processing}}
Neural networks have been used for implementing language models since the early 2000s. and word embedding. Word embedding, such as ''word2vec'', can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, named-entity recognition (token classification), text classification, and others.

Recent developments generalize word embedding to sentence embedding.

Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation|Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples". GT uses English as an intermediate between most language pairs. Research has explored use of deep learning to predict the biomolecular targets, AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.

