anchor|In computing}}

In science fiction, an artificially intelligent computer or robot, even though it has not been programmed with human emotions, often spontaneously experiences those emotions anyway: for example, Agent Smith in ''The Matrix (film)|The Matrix'' was influenced by a "disgust" toward humanity. This is an example of anthropomorphism: in reality, while an artificial intelligence could perhaps be deliberately programmed with human emotions or could develop something similar to an emotion as a means to an ultimate goal ''if'' it is useful to do so, it would not spontaneously develop human emotions for no purpose whatsoever, as portrayed in fiction.

One example of anthropomorphism would be to believe that one's computer is angry at them because they insulted it; another would be to believe that an intelligent robot would naturally find a woman attractive and be driven to mate with her. Scholars sometimes disagree with each other about whether a particular prediction about an artificial intelligence's behavior is logical, or whether the prediction constitutes illogical anthropomorphism.

The conscious use of anthropomorphic metaphor is not intrinsically unwise; ascribing mental processes to the computer, under the proper circumstances, may serve the same purpose as it does when humans do it to other people: it may help persons to understand what the computer will do, how their actions will affect the computer, how to compare computers with humans, and conceivably how to design computer programs. However, inappropriate use of anthropomorphic metaphors can result in false beliefs about the behavior of computers, for example by causing people to overestimate how "flexible" computers are. According to Paul R. Cohen and Edward Feigenbaum, in order to differentiate between anthropomorphization and logical prediction of AI behavior, "the trick is to know enough about how humans and computers think to say ''exactly'' what they have in common, and, when we lack this knowledge, to use the comparison to ''suggest'' theories of human thinking or computer thinking."

Computers overturn the childhood hierarchical taxonomy of "stones (non-living) → plants (living) → animals (conscious) → humans (rational)", by introducing a non-human "actor" that appears to regularly behave rationally. Much of computing terminology derives from anthropomorphic metaphors: computers can "read", "write", or "catch a virus". Information technology presents no clear correspondence with any other entities in the world besides humans; the options are either to leverage an emotional, imprecise human metaphor, or to reject imprecise metaphor and make use of more precise, domain-specific technical terms. This may allow incorporation of anthropomorphic features into computers/robots to enable more familiar "social" interactions, making them easier to use.

Alleged examples of anthropomorphism toward AI have included: Google engineer Blake Lemoine's widely derided 2022 claim that the Google LaMDA chatbot was artificial consciousness|sentient; the 2017 granting of honorary Saudi Arabian citizenship to the robot Sophia (robot)|Sophia; and the reactions to the chatbot ELIZA in the 1960s.

