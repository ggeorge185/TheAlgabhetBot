File: Reinforcement learning diagram.svg|thumb|right|250px| The typical framing of a Reinforcement Learning (RL) scenario: an agent takes actions in an environment, which is interpreted into a reward and a representation of the state, which are fed back into the agent.

Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called ''approximate dynamic programming,'' or ''neuro-dynamic programming.'' The problems of interest in reinforcement learning have also been studied in the optimal control theory|theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment.

Basic reinforcement learning is modeled as a Markov decision process:

* a set of environment and agent states, <math>\mathcal</math>;
* a set of actions, <math>\mathcal</math>, of the agent;
* <math>P_a(s,s')=\Pr(S_=s'\mid S_t=s, A_t=a)</math>, the probability of transition (at time <math>t</math>) from state <math>s</math> to state <math>s'</math> under action <math>a</math>.
* <math>R_a(s,s')</math>, the immediate reward after transition from <math>s</math> to <math>s'</math> with action <math>a</math>.

The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the "reward function" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.

A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time , the agent receives the current state <math>S_t</math> and reward <math>R_t</math>. It then chooses an action <math>A_t</math> from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state <math>S_</math> and the reward <math>R_</math> associated with the ''transition'' <math>(S_t,A_t,S_)</math> is determined. The goal of a reinforcement learning agent is to learn a ''policy'': <math>\pi: \mathcal \times \mathcal \rightarrow [0,1] </math>, <math>\pi(s,a) = \Pr(A_t = a\mid S_t =s)</math> that maximizes the expected cumulative reward.

Formulating the problem as an Markov decision process assumes the agent directly observes the current environmental state; in this case the problem is said to have ''full observability''. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have ''partial observability'', and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.

When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of ''regret (game theory)|regret''. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.

Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation, robot control, photovoltaic generators dispatch, backgammon, checkers, Go (game)|Go (AlphaGo), and autonomous driving systems.

Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:
* A model of the environment is known, but an Closed-form expression|analytic solution is not available;
* Only a simulation model of the environment is given (the subject of simulation-based optimization); 
* The only way to collect information about the environment is to interact with it.
The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.

